{
  "A0065": {
    "title": "大模型安全防护",
    "category": "AC01",
    "definition": "大模型安全防护是指针对大语言模型（LLM）服务部署的一系列安全防护措施，包括输入过滤、输出审查、提示词防注入、模型访问控制等，以防止模型被恶意利用或产生有害输出。",
    "description": "大模型安全防护的主要手段包括：①提示词过滤（Prompt Filtering）：对用户输入的提示词进行安全检测，识别并拦截包含注入攻击、越狱尝试、敏感信息探测等恶意意图的输入。②输出审查（Output Guardrails）：对模型生成的内容进行实时审查，过滤包含有害信息、敏感数据、违规内容的输出。③系统提示词加固：通过精心设计的系统提示词（System Prompt）来约束模型行为边界，防止角色扮演攻击和指令覆盖。④模型访问控制：实施细粒度的API访问控制，包括身份认证、速率限制、使用配额管理等。⑤安全沙箱：将模型运行在隔离的沙箱环境中，限制其对外部系统和数据的访问权限。⑥红队测试：定期对模型进行对抗性测试，发现和修复安全漏洞。⑦内容分级：根据用户身份和场景对模型输出进行分级管理。",
    "limitation": "大模型安全防护的局限性包括：①提示注入的多样性使得完全防御极为困难，攻击者可以通过编码、多语言混合、间接注入等方式绕过过滤。②过度严格的安全策略可能导致模型可用性下降，影响正常用户体验。③安全防护措施本身可能引入额外的延迟和计算开销。④模型的黑盒特性使得难以完全预测和控制其行为。⑤新型攻击手法不断涌现，防护策略需要持续更新。",
    "references": [
      {
        "title": "OWASP Top 10 for LLM Applications",
        "link": "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
      },
      {
        "title": "大模型安全实践指南",
        "link": "https://www.nist.gov/artificial-intelligence/ai-risk-management-framework"
      }
    ],
    "updated": "2026-02-27"
  }
}